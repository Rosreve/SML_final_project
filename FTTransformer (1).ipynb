{"cells":[{"cell_type":"code","execution_count":null,"id":"586fa055","metadata":{"id":"586fa055"},"outputs":[],"source":["import math, time, os, sys, gc, random\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","from itertools import product\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, Subset"]},{"cell_type":"markdown","id":"49ac3982","metadata":{"id":"49ac3982"},"source":["Configs"]},{"cell_type":"code","execution_count":null,"id":"134b8e41","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1760584333165,"user":{"displayName":"Sarah Liu","userId":"15845808413963530009"},"user_tz":-660},"id":"134b8e41","outputId":"a78ecaea-766a-4b07-b066-90197c6bec50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}],"source":["RANDOM_STATE = 42\n","def set_seeds(seed=RANDOM_STATE):\n","    random.seed(seed); np.random.seed(seed)\n","    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","set_seeds()\n","\n","OUTER_K = 10\n","INNER_K = 3\n","SCORING  = \"neg_rmse\"   # inner-loop selection: maximize negative mean RMSE\n","BATCH_SIZE = 1024\n","MAX_EPOCHS = 30\n","PATIENCE   = 5\n","VERBOSE = True\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)"]},{"cell_type":"markdown","id":"18962194","metadata":{"id":"18962194"},"source":["Metrics Helpers"]},{"cell_type":"code","execution_count":null,"id":"05f235e8","metadata":{"id":"05f235e8"},"outputs":[],"source":["def rmse(a,b): return float(np.sqrt(np.mean((a-b)**2)))\n","def mae(a,b):  return float(np.mean(np.abs(a-b)))\n","def r2(a,b):\n","    ssr = np.sum((a-b)**2); sst = np.sum((a - np.mean(a))**2)\n","    return float(1.0 - ssr / (sst + 1e-12))"]},{"cell_type":"markdown","id":"01af2761","metadata":{"id":"01af2761"},"source":["Dataset Preparation"]},{"cell_type":"code","execution_count":null,"id":"3e35708c","metadata":{"id":"3e35708c"},"outputs":[],"source":["df = pd.read_csv(\"/content/NEWUS_Accidents_Featured_Engineered.csv\")\n","\n","TARGETS = [\"Severity_count\", \"Severity_mean\", \"Severity_max\"]\n","for t in TARGETS:\n","    assert t in df.columns, f\"df must contain target '{t}'\"\n","# We'll stratify folds by risk_level if present; else fall back to regular KFold\n","HAS_RISK_LEVEL = \"risk_level\" in df.columns\n","\n","# Forbid leakage / IDs / targets in features\n","FORBID = set([\"grid_id\",\"Severity\",\"duration_minutes\",\"Weather_Lag_Mins\"] + TARGETS + ([\"risk_level\"] if HAS_RISK_LEVEL else [])) & set(df.columns)\n"]},{"cell_type":"markdown","id":"b39a3052","metadata":{"id":"b39a3052"},"source":["Feature Grouping"]},{"cell_type":"code","execution_count":null,"id":"71cf063c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1760584375070,"user":{"displayName":"Sarah Liu","userId":"15845808413963530009"},"user_tz":-660},"id":"71cf063c","outputId":"792ab911-de40-4199-ce03-9d86eebd1eef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Group sizes: {'weather': 9, 'time': 16, 'poi': 13, 'embed': 50, 'city_oh': 30, 'county_oh': 30, 'wcat_oh': 8}\n"]}],"source":["def detect_groups(df: pd.DataFrame):\n","    cols = set(df.columns)\n","    weather_cols = [c for c in [\n","        'Distance(mi)', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)',\n","        'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)', 'Has_Precipitation'\n","    ] if c in cols and c not in FORBID]\n","\n","    time_cols = [c for c in [\n","        'Start_Hour_sin','Start_Hour_cos','Start_DayOfWeek_sin','Start_DayOfWeek_cos',\n","        'Start_Month_sin','Start_Month_cos','Sunrise_Sunset_binary','Civil_Twilight_binary',\n","        'Nautical_Twilight_binary','Astronomical_Twilight_binary','is_Weekend',\n","        'is_RushHour_Morning','is_RushHour_Evening','is_Winter','Wind_Direction_sin','Wind_Direction_cos'\n","    ] if c in cols and c not in FORBID]\n","\n","    poi_cols = [c for c in [\n","        'Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop',\n","        'Traffic_Calming','Traffic_Signal','Turning_Loop'\n","    ] if c in cols and c not in FORBID]\n","\n","    emb_cols = [c for c in df.columns if c.startswith(\"Emb_PCA_\") and c not in FORBID]\n","    city_oh  = [c for c in df.columns if c.startswith(\"City_\")   and c not in FORBID]\n","    county_oh= [c for c in df.columns if c.startswith(\"County_\") and c not in FORBID]\n","    wcat_oh  = [c for c in df.columns if c.startswith(\"Weather_\") and (not c.startswith(\"Emb_\")) and c not in FORBID]\n","    return {\"weather\":weather_cols,\"time\":time_cols,\"poi\":poi_cols,\"embed\":emb_cols,\n","            \"city_oh\":city_oh,\"county_oh\":county_oh,\"wcat_oh\":wcat_oh}\n","\n","GROUPS = detect_groups(df)\n","print(\"Group sizes:\", {k: len(v) for k,v in GROUPS.items()})"]},{"cell_type":"markdown","id":"63ecfc04","metadata":{"id":"63ecfc04"},"source":["Manual cross-validation"]},{"cell_type":"code","execution_count":null,"id":"3bac10a8","metadata":{"id":"3bac10a8"},"outputs":[],"source":["def stratified_kfold_indices(y, n_splits=5, random_state=42):\n","    rng = np.random.RandomState(random_state)\n","    y = np.asarray(y)\n","    classes, y_idx = np.unique(y, return_inverse=True)\n","    per_class = []\n","    for c in range(len(classes)):\n","        idx = np.where(y_idx == c)[0]; rng.shuffle(idx); per_class.append(idx)\n","    folds = [[] for _ in range(n_splits)]\n","    for arr in per_class:\n","        for i, j in enumerate(arr):\n","            folds[i % n_splits].append(j)\n","    all_idx = np.arange(len(y))\n","    pairs = []\n","    for k in range(n_splits):\n","        te = np.array(sorted(folds[k]), dtype=int)\n","        tr = np.setdiff1d(all_idx, te, assume_unique=False)\n","        pairs.append((tr, te))\n","    return pairs\n","\n","def kfold_indices(n, n_splits=5, random_state=42):\n","    rng = np.random.RandomState(random_state)\n","    idx = np.arange(n); rng.shuffle(idx)\n","    folds = np.array_split(idx, n_splits)\n","    pairs = []\n","    all_idx = np.arange(n)\n","    for k in range(n_splits):\n","        te = np.array(sorted(folds[k]), dtype=int)\n","        tr = np.setdiff1d(all_idx, te, assume_unique=False)\n","        pairs.append((tr, te))\n","    return pairs"]},{"cell_type":"markdown","id":"a4aec431","metadata":{"id":"a4aec431"},"source":["Define Blocks"]},{"cell_type":"code","execution_count":null,"id":"f40a3111","metadata":{"id":"f40a3111"},"outputs":[],"source":["def onehot_to_id(mat_oh):\n","    if mat_oh.size == 0: return np.zeros((mat_oh.shape[0],), dtype=np.int64)\n","    argmax = mat_oh.argmax(axis=1) + 1\n","    has_any = (mat_oh.sum(axis=1) > 0).astype(np.int64)\n","    return argmax * has_any\n","\n","def fit_medians(X):\n","    med = np.nanmedian(X, axis=0); med = np.where(np.isnan(med), 0.0, med); return med\n","def apply_medians(X, med): return np.where(np.isnan(X), med, X)\n","\n","def build_numpy_blocks_strict(df_part: pd.DataFrame, groups):\n","    # 1) CONCAT all continuous columns -> one numeric matrix\n","    all_num_cols = groups[\"weather\"] + groups[\"time\"] + groups[\"poi\"] + groups[\"embed\"]\n","    X_num = df_part[all_num_cols].astype(float).values if len(all_num_cols) else np.zeros((len(df_part),0),dtype=np.float32)\n","\n","    # 2) One-hot groups -> IDs (0 = none/other)\n","    out = {\"num\": X_num, \"num_cols\": all_num_cols}\n","    for block in [\"city_oh\",\"county_oh\",\"wcat_oh\"]:\n","        cols = groups[block]\n","        if len(cols):\n","            mat = df_part[cols].astype(float).values\n","            out[block.replace(\"_oh\",\"_id\")] = onehot_to_id(mat)\n","            out[block+\"_K\"] = len(cols)\n","        else:\n","            out[block.replace(\"_oh\",\"_id\")] = np.zeros((len(df_part),), dtype=np.int64)\n","            out[block+\"_K\"] = 0\n","    return out\n"]},{"cell_type":"markdown","id":"a98db4f2","metadata":{"id":"a98db4f2"},"source":["Dataset"]},{"cell_type":"code","execution_count":null,"id":"62b1edc5","metadata":{"id":"62b1edc5"},"outputs":[],"source":["class TabDataset(Dataset):\n","    def __init__(self, blocks, y_norm):\n","        self.blocks = blocks\n","        self.y_norm = y_norm\n","    def __len__(self): return len(self.y_norm)\n","    def __getitem__(self, i):\n","        sample = {\n","            \"num\": self.blocks[\"num\"][i],                 # (n_num,)\n","            \"city_id\":   int(self.blocks[\"city_id\"][i]),\n","            \"county_id\": int(self.blocks[\"county_id\"][i]),\n","            \"wcat_id\":   int(self.blocks[\"wcat_id\"][i]),\n","        }\n","        return sample, self.y_norm[i]\n","\n","def collate_fn(batch):\n","    Xs, ys = zip(*batch)\n","    num      = torch.tensor(np.stack([x[\"num\"] for x in Xs], 0), dtype=torch.float32)       # (B, n_num)\n","    city_id   = torch.tensor([x[\"city_id\"]   for x in Xs], dtype=torch.long)\n","    county_id = torch.tensor([x[\"county_id\"] for x in Xs], dtype=torch.long)\n","    wcat_id   = torch.tensor([x[\"wcat_id\"]   for x in Xs], dtype=torch.long)\n","    y = torch.tensor(np.stack(ys, 0), dtype=torch.float32)\n","    return {\"num\": num, \"city_id\": city_id, \"county_id\": county_id, \"wcat_id\": wcat_id}, y\n","\n","class NumFeatureTokenizer(nn.Module):\n","    \"\"\"\n","    Strict FT-Transformer tokenizer: one token per numeric feature.\n","    token_j = x_j * w_j + b_j   (broadcasted over batch)\n","    \"\"\"\n","    def __init__(self, n_num, d_token):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.randn(n_num, d_token) * 0.02)\n","        self.bias   = nn.Parameter(torch.zeros(n_num, d_token))\n","    def forward(self, x_num):                 # x_num: (B, n_num)\n","        return x_num.unsqueeze(-1) * self.weight + self.bias   # (B, n_num, d_token)\n"]},{"cell_type":"markdown","id":"807ff5dd","metadata":{"id":"807ff5dd"},"source":["FT-Transformer"]},{"cell_type":"code","execution_count":null,"id":"83ed7f8d","metadata":{"id":"83ed7f8d"},"outputs":[],"source":["class FTTransformer(nn.Module):\n","    def __init__(self,\n","                 n_num,                 # number of numeric features\n","                 d_token=160, n_layers=2, n_heads=8, dropout=0.1,\n","                 K_city=0, K_county=0, K_wcat=0,\n","                 n_outputs=3):\n","        super().__init__()\n","        self.d = d_token\n","\n","        # Strict tokenizer: one token per numeric feature\n","        self.num_tok = NumFeatureTokenizer(n_num, d_token)\n","\n","        # Categorical embeddings (+1 for 'none')\n","        self.emb_city   = nn.Embedding(K_city+1,   d_token) if K_city>0 else None\n","        self.emb_county = nn.Embedding(K_county+1, d_token) if K_county>0 else None\n","        self.emb_wcat   = nn.Embedding(K_wcat+1,   d_token) if K_wcat>0 else None\n","\n","        # CLS token\n","        self.cls = nn.Parameter(torch.zeros(1, 1, d_token))\n","        nn.init.trunc_normal_(self.cls, std=0.02)\n","\n","        # Encoder\n","        enc_layer = nn.TransformerEncoderLayer(\n","            d_model=d_token, nhead=n_heads, dim_feedforward=4*d_token,\n","            dropout=dropout, batch_first=True, activation='gelu', norm_first=True\n","        )\n","        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n","\n","        # Head\n","        self.head = nn.Sequential(\n","            nn.LayerNorm(d_token),\n","            nn.Linear(d_token, d_token),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(d_token, n_outputs)\n","        )\n","\n","    def forward(self, xb):\n","        # numeric features -> per-feature tokens\n","        tok_num = self.num_tok(xb[\"num\"])           # (B, n_num, d)\n","\n","        tokens = [tok_num]                          # list of (B, T_i, d)\n","        if self.emb_city   is not None: tokens.append(self.emb_city(xb[\"city_id\"]).unsqueeze(1))     # (B,1,d)\n","        if self.emb_county is not None: tokens.append(self.emb_county(xb[\"county_id\"]).unsqueeze(1)) # (B,1,d)\n","        if self.emb_wcat   is not None: tokens.append(self.emb_wcat(xb[\"wcat_id\"]).unsqueeze(1))     # (B,1,d)\n","\n","        x = torch.cat(tokens, dim=1)                # (B, T_total, d)\n","        cls = self.cls.expand(x.size(0), 1, self.d)\n","        x = torch.cat([cls, x], dim=1)              # prepend CLS\n","        x = self.encoder(x)\n","        cls_out = x[:, 0, :]\n","        y_hat = self.head(cls_out)                  # (B, 3)\n","        return y_hat\n"]},{"cell_type":"markdown","id":"7894f479","metadata":{"id":"7894f479"},"source":["Train and Evaluation"]},{"cell_type":"code","execution_count":null,"id":"922faea1","metadata":{"id":"922faea1"},"outputs":[],"source":["def train_one(model, loader, optimizer, criterion):\n","    model.train(); total = 0.0\n","    for xb, yb in loader:\n","        for k in xb: xb[k] = xb[k].to(device, non_blocking=True)\n","        yb = yb.to(device, non_blocking=True)                 # (B,3) normalized targets\n","        optimizer.zero_grad(set_to_none=True)\n","        pred = model(xb)                                      # (B,3) normalized preds\n","        loss = criterion(pred, yb)\n","        loss.backward(); optimizer.step()\n","        total += loss.item() * yb.size(0)\n","    return total / len(loader.dataset)\n","\n","@torch.no_grad()\n","def eval_one_norm(model, loader):\n","    model.eval()\n","    preds, trues = [], []\n","    for xb, yb in loader:\n","        for k in xb: xb[k] = xb[k].to(device, non_blocking=True)\n","        yb = yb.to(device, non_blocking=True)\n","        out = model(xb)        # normalized\n","        preds.append(out.detach().cpu().numpy())\n","        trues.append(yb.detach().cpu().numpy())\n","    y_pred_norm = np.vstack(preds)   # (N,3)\n","    y_true_norm = np.vstack(trues)   # (N,3)\n","    return y_true_norm, y_pred_norm\n","\n","# Utilities to (de)normalize + log transforms for count\n","def apply_log1p_count(y):      # y: (N,3) in ORIGINAL space\n","    y2 = y.copy()\n","    y2[:,0] = np.log1p(np.clip(y2[:,0], a_min=0, a_max=None))\n","    return y2\n","\n","def invert_log1p_count(y_proc):  # y_proc: (N,3) in PROCESSED (log-count) space\n","    y2 = y_proc.copy()\n","    y2[:,0] = np.expm1(y2[:,0])\n","    return y2\n","\n","def fit_y_scaler(y_proc):  # mean/std per target on TRAIN (after log1p for count)\n","    mu = y_proc.mean(axis=0); sd = y_proc.std(axis=0)\n","    sd = np.where(sd<1e-8, 1.0, sd)\n","    return mu, sd\n","\n","def norm_y(y_proc, mu, sd): return (y_proc - mu) / sd\n","def denorm_y(y_norm, mu, sd): return y_norm * sd + mu\n","\n","def reg_metrics_all(y_true_orig, y_pred_orig):\n","    # returns per-target and averaged metrics in ORIGINAL space (count already inverted)\n","    names = [\"count\",\"mean\",\"max\"]\n","    per = {}\n","    for j, name in enumerate(names):\n","        per[f\"rmse_{name}\"] = rmse(y_true_orig[:,j], y_pred_orig[:,j])\n","        per[f\"mae_{name}\"]  = mae (y_true_orig[:,j], y_pred_orig[:,j])\n","        per[f\"r2_{name}\"]   = r2  (y_true_orig[:,j], y_pred_orig[:,j])\n","    per[\"rmse_avg\"] = np.mean([per[\"rmse_count\"], per[\"rmse_mean\"], per[\"rmse_max\"]])\n","    per[\"mae_avg\"]  = np.mean([per[\"mae_count\"],  per[\"mae_mean\"],  per[\"mae_max\"]])\n","    per[\"r2_avg\"]   = np.mean([per[\"r2_count\"],   per[\"r2_mean\"],   per[\"r2_max\"]])\n","    return per\n","\n","def scoring_value_reg(y_true_norm, y_pred_norm, mu, sd):\n","    # compute mean NEGATIVE RMSE (maximize) in original space\n","    y_true_proc = denorm_y(y_true_norm, mu, sd)\n","    y_pred_proc = denorm_y(y_pred_norm, mu, sd)\n","    y_true_orig = invert_log1p_count(y_true_proc)\n","    y_pred_orig = invert_log1p_count(y_pred_proc)\n","    m = reg_metrics_all(y_true_orig, y_pred_orig)\n","    return -m[\"rmse_avg\"]\n","\n","# ----------------------- Build master X blocks & Y -----------------------\n","X_blocks_all = build_numpy_blocks_strict(df.drop(columns=list(FORBID)), GROUPS)\n","# The columns that went into the 'num' block (exclude one-hots used to form IDs + forbidden)\n","NUMERIC_FEATURE_NAMES = [\n","    c for c in df.drop(columns=list(FORBID)).columns\n","    if not (c.startswith(\"City_\") or c.startswith(\"County_\") or (c.startswith(\"Weather_\") and not c.startswith(\"Emb_\")))\n","]\n","assert X_blocks_all[\"num\"].shape[1] == len(NUMERIC_FEATURE_NAMES), \\\n","    f\"num block width {X_blocks_all['num'].shape[1]} != names {len(NUMERIC_FEATURE_NAMES)}\"\n","\n","Y_all_raw = df[TARGETS].astype(float).values  # ORIGINAL space\n","\n","N = len(Y_all_raw)\n","if HAS_RISK_LEVEL:\n","    y_split = df[\"risk_level\"].astype(int).values\n","    outer_pairs = stratified_kfold_indices(y_split, n_splits=OUTER_K, random_state=RANDOM_STATE)\n","else:\n","    outer_pairs = kfold_indices(N, n_splits=OUTER_K, random_state=RANDOM_STATE)"]},{"cell_type":"markdown","id":"fb399b15","metadata":{"id":"fb399b15"},"source":["Feature Importance Utils"]},{"cell_type":"code","execution_count":null,"id":"73717507","metadata":{"id":"73717507"},"outputs":[],"source":["from copy import deepcopy\n","from torch.utils.data import DataLoader\n","\n","def _make_dataset_from_blocks(blocks, y_norm):\n","    \"\"\"Build a minimal dataset compatible with your current collate_fn that expects 'num' and optional IDs.\"\"\"\n","    class ImpDataset(Dataset):\n","        def __init__(self, blocks, y_norm):\n","            self.blocks = blocks\n","            self.y_norm = y_norm\n","        def __len__(self): return len(self.y_norm)\n","        def __getitem__(self, i):\n","            sample = {\"num\": self.blocks[\"num\"][i]}\n","            if \"city_id\" in self.blocks:   sample[\"city_id\"]   = int(self.blocks[\"city_id\"][i])\n","            if \"county_id\" in self.blocks: sample[\"county_id\"] = int(self.blocks[\"county_id\"][i])\n","            if \"wcat_id\" in self.blocks:   sample[\"wcat_id\"]   = int(self.blocks[\"wcat_id\"][i])\n","            return sample, self.y_norm[i]\n","    return ImpDataset(blocks, y_norm)\n","\n","@torch.no_grad()\n","def _score_model_reg(model, blocks, y_norm, mu, sd, collate_fn, device, batch_size=1024):\n","    ds  = _make_dataset_from_blocks(blocks, y_norm)\n","    dl  = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n","    y_true_norm, y_pred_norm = eval_one_norm(model, dl)\n","    # use your existing scorer -> negative RMSE (higher is better)\n","    return scoring_value_reg(y_true_norm, y_pred_norm, mu, sd)\n","\n","def permutation_importance(\n","    model, X_blocks, Y_norm, mu, sd, collate_fn,\n","    block_name=None, feature_names=None, n_repeats=1, device=\"cuda\", seed=42\n","):\n","    \"\"\"\n","    If block_name is None -> block-level importance over available keys: ['num','city_id','county_id','wcat_id'] (those that exist).\n","    If block_name == 'num' and feature_names is provided -> per-feature importance within the numeric block.\n","    Returns (base_score, importance_df)\n","    \"\"\"\n","    rng = np.random.RandomState(seed)\n","\n","    # ----- baseline -----\n","    base = _score_model_reg(model, X_blocks, Y_norm, mu, sd, collate_fn, device)\n","\n","    # ----- block-level mode -----\n","    if block_name is None:\n","        keys = [k for k in [\"num\",\"city_id\",\"county_id\",\"wcat_id\"] if k in X_blocks]\n","        rows = []\n","        for k in keys:\n","            worst = []\n","            for _ in range(n_repeats):\n","                Xp = deepcopy(X_blocks)\n","                arr = Xp[k]\n","                if arr.ndim == 1:  # IDs\n","                    rng.shuffle(arr)\n","                else:               # full matrix -> shuffle rows jointly\n","                    idx = rng.permutation(arr.shape[0])\n","                    Xp[k] = arr[idx]\n","                score = _score_model_reg(model, Xp, Y_norm, mu, sd, collate_fn, device)\n","                worst.append(base - score)  # drop in score = importance\n","            rows.append((k, float(np.mean(worst))))\n","        imp_df = pd.DataFrame(rows, columns=[\"block\",\"importance\"]).sort_values(\"importance\", ascending=False)\n","        return base, imp_df\n","\n","    # ----- per-feature mode: only for 'num' -----\n","    if block_name == \"num\":\n","        assert feature_names is not None and len(feature_names) == X_blocks[\"num\"].shape[1], \\\n","            \"Provide feature_names for the 'num' block with correct length.\"\n","        rows = []\n","        for j, name in enumerate(feature_names):\n","            drops = []\n","            for _ in range(n_repeats):\n","                Xp = deepcopy(X_blocks)\n","                col = Xp[\"num\"][:, j].copy()\n","                rng.shuffle(col)\n","                Xp[\"num\"][:, j] = col\n","                score = _score_model_reg(model, Xp, Y_norm, mu, sd, collate_fn, device)\n","                drops.append(base - score)\n","            rows.append((name, float(np.mean(drops))))\n","        imp_df = pd.DataFrame(rows, columns=[\"feature\",\"importance\"]).sort_values(\"importance\", ascending=False)\n","        return base, imp_df\n","\n","    raise ValueError(f\"Unsupported block_name='{block_name}' for permutation_importance()\")\n"]},{"cell_type":"markdown","id":"24a1e446","metadata":{"id":"24a1e446"},"source":["###Outer loop"]},{"cell_type":"markdown","id":"ShlPr2koqNBa","metadata":{"id":"ShlPr2koqNBa"},"source":["hyperparameter tuing 1"]},{"cell_type":"code","execution_count":null,"id":"14e57424","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5504586,"status":"ok","timestamp":1760514951760,"user":{"displayName":"Sarah Liu","userId":"15845808413963530009"},"user_tz":-660},"id":"14e57424","outputId":"4c1d82a5-0c3e-4f18-af7d-9b22cae433fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting nested CV (multi-task regression)...\n","\n","==== Outer fold 1/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2446 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2509 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2388 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2410 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2498 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2417 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2457 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2518 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2463 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2388 with {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 2/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2526 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2481 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2393 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2557 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2505 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2391 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2454 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2537 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2369 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2369 with {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 3/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2445 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2531 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2405 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2482 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2486 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2423 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2501 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2508 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2426 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2405 with {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 4/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2453 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2492 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2414 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2463 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2567 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2415 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2509 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2688 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2418 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2414 with {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 5/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2476 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2490 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2376 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2467 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2503 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2429 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2500 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2526 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2411 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2376 with {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 6/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2453 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2531 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2384 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2465 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2512 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2388 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2511 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2522 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2416 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2384 with {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 7/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2422 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2459 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2348 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2478 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2523 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2407 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2478 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2488 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2402 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2348 with {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 8/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2493 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2548 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2393 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2492 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2465 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2413 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2486 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2508 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2488 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2393 with {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 9/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2451 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2477 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2365 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2458 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2554 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2476 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2480 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2555 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2405 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2365 with {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 10/10 ====\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2441 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2426 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2393 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2439 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2538 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2383 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.0015, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2490 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.002, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2685 RMSE (lower is better)\n","  Params {'d_token': 192, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2448 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2383 with {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n"]}],"source":["# Hyper Parameter Tuning\n","param_grid = {\n","    \"d_token\":  [144, 160, 192],\n","    \"n_layers\": [1],\n","    \"lr\":       [1.5e-3, 2e-3, 1e-3],\n","    \"dropout\":  [0.1],\n","    \"n_heads\":  [8],  # adjust if include d_token not divisible by 8\n","    \"weight_decay\": [1e-6],\n","}\n","\n","def param_grid_iter(grid: dict):\n","    keys = list(grid.keys()); vals = [grid[k] for k in keys]\n","    for combo in product(*vals):\n","        yield dict(zip(keys, combo))\n","\n","# ----------------------- Outer loop -----------------------\n","outer_results = []\n","best_params_all = []\n","# Collectors for importance across folds\n","imp_blocks_all = []\n","imp_weather_all = []\n","saliency_all = []   # Grad×Input (example for 'weather')\n","\n","\n","def make_X(idx):\n","    return {k: (v[idx].copy() if isinstance(v, np.ndarray) else v) for k,v in X_blocks_all.items()}\n","\n","print(\"Starting nested CV (multi-task regression)...\")\n","for fold_id, (tr_idx, te_idx) in enumerate(outer_pairs, start=1):\n","    if VERBOSE: print(f\"\\n==== Outer fold {fold_id}/{OUTER_K} ====\")\n","\n","    # ----- Build X blocks (train/test) & impute continuous blocks with train medians -----\n","    X_tr = make_X(tr_idx); X_te = make_X(te_idx)\n","    def impute_block(arr, med=None):\n","        if arr.shape[1]==0: return arr\n","        if med is None: med = fit_medians(arr)\n","        return apply_medians(arr, med)\n","\n","    med_num = fit_medians(X_tr[\"num\"]) if X_tr[\"num\"].shape[1] else None\n","    def impute_num(arr, med=None):\n","        if arr.shape[1] == 0: return arr\n","        if med is None: med = fit_medians(arr)\n","        return apply_medians(arr, med)\n","\n","    X_tr[\"num\"] = impute_num(X_tr[\"num\"], med_num)\n","    X_te[\"num\"] = impute_num(X_te[\"num\"], med_num)\n","\n","    # ----- Build Y (train/test) -----\n","    Y_tr_raw = Y_all_raw[tr_idx]     # ORIGINAL\n","    Y_te_raw = Y_all_raw[te_idx]\n","\n","    # Apply log1p to count target for training stability -> \"processed\" space\n","    Y_tr_proc = apply_log1p_count(Y_tr_raw)\n","    Y_te_proc = apply_log1p_count(Y_te_raw)\n","\n","    # Standardize per target using OUTER-TRAIN only\n","    mu, sd = fit_y_scaler(Y_tr_proc)\n","    Y_tr_norm = norm_y(Y_tr_proc, mu, sd)\n","    Y_te_norm = norm_y(Y_te_proc, mu, sd)   # used only to pack datasets consistently (not for metrics)\n","\n","    # ----- Inner CV splits on y_split[tr_idx] or plain KFold -----\n","    if HAS_RISK_LEVEL:\n","        inner_pairs = stratified_kfold_indices(df[\"risk_level\"].values[tr_idx], n_splits=INNER_K, random_state=RANDOM_STATE+fold_id)\n","    else:\n","        inner_pairs = kfold_indices(len(tr_idx), n_splits=INNER_K, random_state=RANDOM_STATE+fold_id)\n","\n","    best_score, best_p = -1e9, None\n","\n","    for p in param_grid_iter(param_grid):\n","        inner_scores = []\n","        for in_tr_ind, in_val_ind in inner_pairs:\n","            # slice X and Y (normalized) for inner split\n","            in_tr_blocks = {k: v[in_tr_ind] if isinstance(v, np.ndarray) else v for k,v in X_tr.items()}\n","            in_val_blocks= {k: v[in_val_ind] if isinstance(v, np.ndarray) else v for k,v in X_tr.items()}\n","            y_in_tr = Y_tr_norm[in_tr_ind]; y_in_val = Y_tr_norm[in_val_ind]\n","\n","            tr_ds = TabDataset(in_tr_blocks, y_in_tr); va_ds = TabDataset(in_val_blocks, y_in_val)\n","            tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, collate_fn=collate_fn)\n","            va_loader = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n","\n","            model = FTTransformer(\n","                      n_num=X_tr[\"num\"].shape[1],\n","                      d_token=p[\"d_token\"], n_layers=p[\"n_layers\"], n_heads=p[\"n_heads\"], dropout=p[\"dropout\"],\n","                      K_city=X_tr.get(\"city_oh_K\",0), K_county=X_tr.get(\"county_oh_K\",0), K_wcat=X_tr.get(\"wcat_oh_K\",0),\n","                      n_outputs=3\n","                  ).to(device)\n","\n","\n","\n","            optimizer = torch.optim.AdamW(model.parameters(), lr=p[\"lr\"], weight_decay=p[\"weight_decay\"])\n","            criterion = nn.MSELoss()\n","\n","            best_val = -1e9; no_improve = 0\n","            for epoch in range(1, MAX_EPOCHS+1):\n","                _ = train_one(model, tr_loader, optimizer, criterion)\n","                y_val_true_norm, y_val_pred_norm = eval_one_norm(model, va_loader)\n","                score = scoring_value_reg(y_val_true_norm, y_val_pred_norm, mu, sd)  # uses ORIGINAL-scale RMSE (avg)\n","\n","                if score > best_val + 1e-6:\n","                    best_val = score; no_improve = 0\n","                    best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n","                else:\n","                    no_improve += 1\n","                if no_improve >= PATIENCE: break\n","\n","            model.load_state_dict(best_state)\n","            # final inner val score\n","            y_val_true_norm, y_val_pred_norm = eval_one_norm(model, va_loader)\n","            inner_scores.append(scoring_value_reg(y_val_true_norm, y_val_pred_norm, mu, sd))\n","\n","        mean_inner = float(np.mean(inner_scores))\n","        if VERBOSE: print(f\"  Params {p} -> inner mean {-mean_inner:.4f} RMSE (lower is better)\")\n","        if mean_inner > best_score:\n","            best_score, best_p = mean_inner, p\n","\n","    best_params_all.append(best_p)\n","    if VERBOSE: print(f\"Best inner (mean RMSE): {-best_score:.4f} with {best_p}\")\n","\n","    # ----- Retrain on FULL outer-train with best params (with small ES split) -----\n","    tr_ds_full = TabDataset(X_tr, Y_tr_norm); te_ds = TabDataset(X_te, Y_te_norm)\n","    tr_loader_full = DataLoader(tr_ds_full, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, collate_fn=collate_fn)\n","    te_loader      = DataLoader(te_ds,      batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n","\n","    model = FTTransformer(\n","        n_num=X_tr[\"num\"].shape[1],\n","        d_token=best_p[\"d_token\"], n_layers=best_p[\"n_layers\"], n_heads=best_p[\"n_heads\"], dropout=best_p[\"dropout\"],\n","        K_city=X_tr.get(\"city_oh_K\",0), K_county=X_tr.get(\"county_oh_K\",0), K_wcat=X_tr.get(\"wcat_oh_K\",0),\n","        n_outputs=3\n","    ).to(device)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=best_p[\"lr\"], weight_decay=best_p[\"weight_decay\"])\n","    criterion = nn.MSELoss()\n","\n","    idx_all = np.arange(len(tr_idx)); np.random.shuffle(idx_all)\n","    cut = max(1, int(0.1*len(idx_all)))\n","    es_val = idx_all[:cut]; es_tr = idx_all[cut:]\n","    es_tr_loader = DataLoader(Subset(tr_ds_full, es_tr), batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n","    es_va_loader = DataLoader(Subset(tr_ds_full, es_val), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n","\n","    best_val = -1e9; no_improve = 0\n","    for epoch in range(1, MAX_EPOCHS+1):\n","        _ = train_one(model, es_tr_loader, optimizer, criterion)\n","        y_val_true_norm, y_val_pred_norm = eval_one_norm(model, es_va_loader)\n","        score = scoring_value_reg(y_val_true_norm, y_val_pred_norm, mu, sd)\n","        if score > best_val + 1e-6:\n","            best_val = score; no_improve = 0\n","            best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n","        else:\n","            no_improve += 1\n","        if no_improve >= PATIENCE: break\n","    model.load_state_dict(best_state)\n","\n","    # ----- Evaluate on OUTER-TEST in ORIGINAL space -----\n","    y_te_true_norm, y_te_pred_norm = eval_one_norm(model, te_loader)\n","    y_te_true_proc = denorm_y(y_te_true_norm, mu, sd)\n","    y_te_pred_proc = denorm_y(y_te_pred_norm, mu, sd)\n","    y_te_true_orig = invert_log1p_count(y_te_true_proc)\n","    y_te_pred_orig = invert_log1p_count(y_te_pred_proc)\n","\n","    m = reg_metrics_all(y_te_true_orig, y_te_pred_orig)\n","    m[\"fold\"] = fold_id\n","    outer_results.append(m)\n","\n","    # --- Feature importance on OUTER-TEST for this fold ---\n","    # (A) Block-level permutation importance over ['num', 'city_id', 'county_id', 'wcat_id'] (whatever exists)\n","    base_metrics_imp, imp_blocks = permutation_importance(\n","        model, X_te, Y_te_norm, mu, sd, collate_fn,\n","        block_name=None, device=device\n","    )\n","    imp_blocks[\"fold\"] = fold_id\n","    imp_blocks_all.append(imp_blocks)\n","\n","    # (B) Per-feature permutation inside the numeric block\n","    if X_te[\"num\"].shape[1] > 0:\n","        _, imp_num = permutation_importance(\n","            model, X_te, Y_te_norm, mu, sd, collate_fn,\n","            block_name=\"num\", feature_names=NUMERIC_FEATURE_NAMES, device=device\n","        )\n","        imp_num[\"fold\"] = fold_id\n","        imp_weather_all.append(imp_num)   # or rename this collector to imp_num_all\n"]},{"cell_type":"markdown","id":"4d2a3d76","metadata":{"id":"4d2a3d76"},"source":["Report 1"]},{"cell_type":"code","execution_count":null,"id":"3b05e76f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1760514951787,"user":{"displayName":"Sarah Liu","userId":"15845808413963530009"},"user_tz":-660},"id":"3b05e76f","outputId":"92871a4f-d11c-4fda-dfce-addb218ca3b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Results by fold (REGRESSION):\n","      rmse_count  r2_count  rmse_mean   r2_mean  rmse_max    r2_max  rmse_avg  \\\n","fold                                                                            \n","1       0.045984  0.820209   0.064566  0.442462  0.595408  0.485413  0.235319   \n","2       0.049296  0.791592   0.069477  0.467236  0.635808  0.463234  0.251527   \n","3       0.046567  0.818129   0.060587  0.478808  0.588562  0.491241  0.231905   \n","4       0.046632  0.810017   0.066050  0.540523  0.622165  0.503153  0.244949   \n","5       0.048003  0.798642   0.063633  0.484587  0.621159  0.464156  0.244265   \n","6       0.044544  0.829398   0.060543  0.543613  0.609585  0.490584  0.238224   \n","7       0.049904  0.780022   0.064174  0.480672  0.630515  0.419564  0.248198   \n","8       0.046207  0.819106   0.061453  0.533168  0.570610  0.529049  0.226090   \n","9       0.048198  0.790386   0.068018  0.434531  0.618926  0.444314  0.245047   \n","10      0.049965  0.793837   0.067004  0.546350  0.587581  0.515689  0.234850   \n","\n","        r2_avg  \n","fold            \n","1     0.582694  \n","2     0.574021  \n","3     0.596059  \n","4     0.617898  \n","5     0.582462  \n","6     0.621198  \n","7     0.560086  \n","8     0.627107  \n","9     0.556410  \n","10    0.618625  \n","\n","Mean ± SE (original scale):\n","rmse_count: 0.0475 ± 0.0006\n"," rmse_mean: 0.0646 ± 0.0010\n","  rmse_max: 0.6080 ± 0.0068\n","  rmse_avg: 0.2400 ± 0.0025\n"," mae_count: 0.0374 ± 0.0006\n","  mae_mean: 0.0297 ± 0.0008\n","   mae_max: 0.3753 ± 0.0092\n","   mae_avg: 0.1475 ± 0.0034\n","  r2_count: 0.8051 ± 0.0052\n","   r2_mean: 0.4952 ± 0.0135\n","    r2_max: 0.4806 ± 0.0105\n","    r2_avg: 0.5937 ± 0.0083\n","\n","Most selected hyperparameters:\n","- d_token: 144\n","- n_layers: 1\n","- lr: 0.001\n","- dropout: 0.1\n","- n_heads: 8\n","- weight_decay: 1e-06\n"]}],"source":["res_df = pd.DataFrame(outer_results).set_index(\"fold\").sort_index()\n","mean = res_df.mean(numeric_only=True); se = res_df.std(ddof=1, numeric_only=True) / np.sqrt(len(res_df))\n","\n","print(\"\\nResults by fold (REGRESSION):\")\n","print(res_df[[c for c in res_df.columns if c.startswith(\"rmse_\") or c.startswith(\"r2_\")]])\n","\n","print(\"\\nMean ± SE (original scale):\")\n","for k in [\"rmse_count\",\"rmse_mean\",\"rmse_max\",\"rmse_avg\",\"mae_count\",\"mae_mean\",\"mae_max\",\"mae_avg\",\"r2_count\",\"r2_mean\",\"r2_max\",\"r2_avg\"]:\n","    print(f\"{k:>10}: {mean.get(k,np.nan):.4f} ± {se.get(k,np.nan):.4f}\")\n","\n","\n","hp_df = pd.DataFrame(best_params_all)\n","print(\"\\nMost selected hyperparameters:\")\n","for col in hp_df.columns:\n","    try:\n","        print(f\"- {col}: {Counter(hp_df[col]).most_common(1)[0][0]}\")\n","    except Exception:\n","        pass"]},{"cell_type":"code","execution_count":null,"id":"64cc0855","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1760515044004,"user":{"displayName":"Sarah Liu","userId":"15845808413963530009"},"user_tz":-660},"id":"64cc0855","outputId":"0049fe95-b9fa-4482-fb81-c62836b0b0ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Block permutation importance (ΔRMSE, mean over folds — higher = more important):\n","block\n","num          0.188599\n","county_id    0.000568\n","city_id      0.000301\n","wcat_id      0.000175\n","Name: importance, dtype: float64\n","\n","Top NUMERIC features by ΔRMSE (mean over folds):\n","feature\n","Emb_PCA_0                       0.073792\n","Emb_PCA_4                       0.055127\n","Emb_PCA_6                       0.028085\n","Emb_PCA_2                       0.010743\n","Emb_PCA_3                       0.006116\n","Emb_PCA_5                       0.005474\n","Emb_PCA_8                       0.004856\n","Emb_PCA_1                       0.004360\n","Bump                            0.003013\n","Amenity                         0.001962\n","Emb_PCA_26                      0.001876\n","Astronomical_Twilight_binary    0.001659\n","Emb_PCA_14                      0.001576\n","Start_Month_sin                 0.001528\n","Emb_PCA_7                       0.001523\n","Start_Hour_sin                  0.001374\n","Wind_Direction_cos              0.001357\n","Emb_PCA_43                      0.001318\n","Emb_PCA_23                      0.001239\n","is_Winter                       0.001199\n","Name: importance, dtype: float64\n"]}],"source":["# ---- Aggregate importance over folds ----\n","if len(imp_blocks_all):\n","    imp_blocks_df = pd.concat(imp_blocks_all, ignore_index=True)\n","    # columns are: ['block', 'importance', 'fold']\n","    print(\"\\nBlock permutation importance (ΔRMSE, mean over folds — higher = more important):\")\n","    print(\n","        imp_blocks_df.groupby(\"block\")[\"importance\"]\n","        .mean()\n","        .sort_values(ascending=False)\n","    )\n","\n","if len(imp_weather_all):\n","    # This list actually holds per-feature importance for the numeric block.\n","    # If you renamed it to imp_num_all, change the variable name accordingly.\n","    imp_num_df = pd.concat(imp_weather_all, ignore_index=True)\n","    # columns are: ['feature', 'importance', 'fold']\n","    print(\"\\nTop NUMERIC features by ΔRMSE (mean over folds):\")\n","    print(\n","        imp_num_df.groupby(\"feature\")[\"importance\"]\n","        .mean()\n","        .sort_values(ascending=False)\n","        .head(20)\n","    )\n","\n","# If you collected Grad×Input saliency per feature into saliency_all (a list of Series),\n","# this still works. Just make sure the Series index are numeric feature names.\n","if len(saliency_all):\n","    saliency_mat = pd.concat(saliency_all, axis=1)  # columns = folds\n","    print(\"\\nGrad×Input saliency (NUM) — mean over folds:\")\n","    print(saliency_mat.mean(axis=1).sort_values(ascending=False).head(20))\n"]},{"cell_type":"markdown","id":"HI5Mseuu1CCz","metadata":{"id":"HI5Mseuu1CCz"},"source":["hyperparameter tuning 2"]},{"cell_type":"code","execution_count":null,"id":"nbPRnlYy1Biz","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbPRnlYy1Biz","executionInfo":{"status":"ok","timestamp":1760599388970,"user_tz":-660,"elapsed":3366676,"user":{"displayName":"Sarah Liu","userId":"15845808413963530009"}},"outputId":"9ea3576e-ee6b-4e53-8d20-cf4a5006bb88"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Starting nested CV (multi-task regression)...\n","\n","==== Outer fold 1/10 ====\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2402 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2383 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2333 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2385 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2381 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2357 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2401 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2393 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2404 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2333 with {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 2/10 ====\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2336 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2357 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2334 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2449 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2355 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2356 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2382 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2393 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2397 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2334 with {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 3/10 ====\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2383 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2404 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2416 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2416 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2393 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2452 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2412 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2422 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2366 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2366 with {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 4/10 ====\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2364 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2338 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2376 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2394 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2359 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2354 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2371 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2393 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2383 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2338 with {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 5/10 ====\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2401 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2352 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2351 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2387 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2360 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2377 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2404 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2397 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2394 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2351 with {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 6/10 ====\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2440 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2358 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2347 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2431 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2367 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2411 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2386 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2369 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2376 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2347 with {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 7/10 ====\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2364 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2321 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2331 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2397 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2364 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2369 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2372 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2431 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2389 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2321 with {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 8/10 ====\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2413 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2368 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2350 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2455 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2431 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2393 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2436 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2424 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2368 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2350 with {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 9/10 ====\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2449 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2375 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2371 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2385 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2389 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2389 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2400 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2420 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2388 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2371 with {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n","\n","==== Outer fold 10/10 ====\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["  Params {'d_token': 128, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2398 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2351 RMSE (lower is better)\n","  Params {'d_token': 128, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2378 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2419 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2372 RMSE (lower is better)\n","  Params {'d_token': 144, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2366 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 1, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2434 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2387 RMSE (lower is better)\n","  Params {'d_token': 160, 'n_layers': 3, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06} -> inner mean 0.2372 RMSE (lower is better)\n","Best inner (mean RMSE): 0.2351 with {'d_token': 128, 'n_layers': 2, 'lr': 0.001, 'dropout': 0.1, 'n_heads': 8, 'weight_decay': 1e-06}\n"]}],"source":["# Hyper Parameter Tuning\n","param_grid = {\n","    \"d_token\":  [112, 128, 144],\n","    \"n_layers\": [2, 3, 4],\n","    \"lr\":       [1e-3],\n","    \"dropout\":  [0.1],\n","    \"n_heads\":  [8],  # adjust if include d_token not divisible by 8\n","    \"weight_decay\": [1e-6],\n","}\n","\n","def param_grid_iter(grid: dict):\n","    keys = list(grid.keys()); vals = [grid[k] for k in keys]\n","    for combo in product(*vals):\n","        yield dict(zip(keys, combo))\n","\n","# ----------------------- Outer loop -----------------------\n","outer_results = []\n","best_params_all = []\n","# Collectors for importance across folds\n","imp_blocks_all = []\n","imp_weather_all = []\n","saliency_all = []   # Grad×Input (example for 'weather')\n","\n","\n","def make_X(idx):\n","    return {k: (v[idx].copy() if isinstance(v, np.ndarray) else v) for k,v in X_blocks_all.items()}\n","\n","print(\"Starting nested CV (multi-task regression)...\")\n","for fold_id, (tr_idx, te_idx) in enumerate(outer_pairs, start=1):\n","    if VERBOSE: print(f\"\\n==== Outer fold {fold_id}/{OUTER_K} ====\")\n","\n","    # ----- Build X blocks (train/test) & impute continuous blocks with train medians -----\n","    X_tr = make_X(tr_idx); X_te = make_X(te_idx)\n","    def impute_block(arr, med=None):\n","        if arr.shape[1]==0: return arr\n","        if med is None: med = fit_medians(arr)\n","        return apply_medians(arr, med)\n","\n","    med_num = fit_medians(X_tr[\"num\"]) if X_tr[\"num\"].shape[1] else None\n","    def impute_num(arr, med=None):\n","        if arr.shape[1] == 0: return arr\n","        if med is None: med = fit_medians(arr)\n","        return apply_medians(arr, med)\n","\n","    X_tr[\"num\"] = impute_num(X_tr[\"num\"], med_num)\n","    X_te[\"num\"] = impute_num(X_te[\"num\"], med_num)\n","\n","    # ----- Build Y (train/test) -----\n","    Y_tr_raw = Y_all_raw[tr_idx]     # ORIGINAL\n","    Y_te_raw = Y_all_raw[te_idx]\n","\n","    # Apply log1p to count target for training stability -> \"processed\" space\n","    Y_tr_proc = apply_log1p_count(Y_tr_raw)\n","    Y_te_proc = apply_log1p_count(Y_te_raw)\n","\n","    # Standardize per target using OUTER-TRAIN only\n","    mu, sd = fit_y_scaler(Y_tr_proc)\n","    Y_tr_norm = norm_y(Y_tr_proc, mu, sd)\n","    Y_te_norm = norm_y(Y_te_proc, mu, sd)   # used only to pack datasets consistently (not for metrics)\n","\n","    # ----- Inner CV splits on y_split[tr_idx] or plain KFold -----\n","    if HAS_RISK_LEVEL:\n","        inner_pairs = stratified_kfold_indices(df[\"risk_level\"].values[tr_idx], n_splits=INNER_K, random_state=RANDOM_STATE+fold_id)\n","    else:\n","        inner_pairs = kfold_indices(len(tr_idx), n_splits=INNER_K, random_state=RANDOM_STATE+fold_id)\n","\n","    best_score, best_p = -1e9, None\n","\n","    for p in param_grid_iter(param_grid):\n","        inner_scores = []\n","        for in_tr_ind, in_val_ind in inner_pairs:\n","            # slice X and Y (normalized) for inner split\n","            in_tr_blocks = {k: v[in_tr_ind] if isinstance(v, np.ndarray) else v for k,v in X_tr.items()}\n","            in_val_blocks= {k: v[in_val_ind] if isinstance(v, np.ndarray) else v for k,v in X_tr.items()}\n","            y_in_tr = Y_tr_norm[in_tr_ind]; y_in_val = Y_tr_norm[in_val_ind]\n","\n","            tr_ds = TabDataset(in_tr_blocks, y_in_tr); va_ds = TabDataset(in_val_blocks, y_in_val)\n","            tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, collate_fn=collate_fn)\n","            va_loader = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n","\n","            model = FTTransformer(\n","                      n_num=X_tr[\"num\"].shape[1],\n","                      d_token=p[\"d_token\"], n_layers=p[\"n_layers\"], n_heads=p[\"n_heads\"], dropout=p[\"dropout\"],\n","                      K_city=X_tr.get(\"city_oh_K\",0), K_county=X_tr.get(\"county_oh_K\",0), K_wcat=X_tr.get(\"wcat_oh_K\",0),\n","                      n_outputs=3\n","                  ).to(device)\n","\n","\n","\n","            optimizer = torch.optim.AdamW(model.parameters(), lr=p[\"lr\"], weight_decay=p[\"weight_decay\"])\n","            criterion = nn.MSELoss()\n","\n","            best_val = -1e9; no_improve = 0\n","            for epoch in range(1, MAX_EPOCHS+1):\n","                _ = train_one(model, tr_loader, optimizer, criterion)\n","                y_val_true_norm, y_val_pred_norm = eval_one_norm(model, va_loader)\n","                score = scoring_value_reg(y_val_true_norm, y_val_pred_norm, mu, sd)  # uses ORIGINAL-scale RMSE (avg)\n","\n","                if score > best_val + 1e-6:\n","                    best_val = score; no_improve = 0\n","                    best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n","                else:\n","                    no_improve += 1\n","                if no_improve >= PATIENCE: break\n","\n","            model.load_state_dict(best_state)\n","            # final inner val score\n","            y_val_true_norm, y_val_pred_norm = eval_one_norm(model, va_loader)\n","            inner_scores.append(scoring_value_reg(y_val_true_norm, y_val_pred_norm, mu, sd))\n","\n","        mean_inner = float(np.mean(inner_scores))\n","        if VERBOSE: print(f\"  Params {p} -> inner mean {-mean_inner:.4f} RMSE (lower is better)\")\n","        if mean_inner > best_score:\n","            best_score, best_p = mean_inner, p\n","\n","    best_params_all.append(best_p)\n","    if VERBOSE: print(f\"Best inner (mean RMSE): {-best_score:.4f} with {best_p}\")\n","\n","    # ----- Retrain on FULL outer-train with best params (with small ES split) -----\n","    tr_ds_full = TabDataset(X_tr, Y_tr_norm); te_ds = TabDataset(X_te, Y_te_norm)\n","    tr_loader_full = DataLoader(tr_ds_full, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, collate_fn=collate_fn)\n","    te_loader      = DataLoader(te_ds,      batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n","\n","    model = FTTransformer(\n","        n_num=X_tr[\"num\"].shape[1],\n","        d_token=best_p[\"d_token\"], n_layers=best_p[\"n_layers\"], n_heads=best_p[\"n_heads\"], dropout=best_p[\"dropout\"],\n","        K_city=X_tr.get(\"city_oh_K\",0), K_county=X_tr.get(\"county_oh_K\",0), K_wcat=X_tr.get(\"wcat_oh_K\",0),\n","        n_outputs=3\n","    ).to(device)\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=best_p[\"lr\"], weight_decay=best_p[\"weight_decay\"])\n","    criterion = nn.MSELoss()\n","\n","    idx_all = np.arange(len(tr_idx)); np.random.shuffle(idx_all)\n","    cut = max(1, int(0.1*len(idx_all)))\n","    es_val = idx_all[:cut]; es_tr = idx_all[cut:]\n","    es_tr_loader = DataLoader(Subset(tr_ds_full, es_tr), batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n","    es_va_loader = DataLoader(Subset(tr_ds_full, es_val), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n","\n","    best_val = -1e9; no_improve = 0\n","    for epoch in range(1, MAX_EPOCHS+1):\n","        _ = train_one(model, es_tr_loader, optimizer, criterion)\n","        y_val_true_norm, y_val_pred_norm = eval_one_norm(model, es_va_loader)\n","        score = scoring_value_reg(y_val_true_norm, y_val_pred_norm, mu, sd)\n","        if score > best_val + 1e-6:\n","            best_val = score; no_improve = 0\n","            best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n","        else:\n","            no_improve += 1\n","        if no_improve >= PATIENCE: break\n","    model.load_state_dict(best_state)\n","\n","    # ----- Evaluate on OUTER-TEST in ORIGINAL space -----\n","    y_te_true_norm, y_te_pred_norm = eval_one_norm(model, te_loader)\n","    y_te_true_proc = denorm_y(y_te_true_norm, mu, sd)\n","    y_te_pred_proc = denorm_y(y_te_pred_norm, mu, sd)\n","    y_te_true_orig = invert_log1p_count(y_te_true_proc)\n","    y_te_pred_orig = invert_log1p_count(y_te_pred_proc)\n","\n","    m = reg_metrics_all(y_te_true_orig, y_te_pred_orig)\n","    m[\"fold\"] = fold_id\n","    outer_results.append(m)\n","\n","    # --- Feature importance on OUTER-TEST for this fold ---\n","    # (A) Block-level permutation importance over ['num', 'city_id', 'county_id', 'wcat_id'] (whatever exists)\n","    base_metrics_imp, imp_blocks = permutation_importance(\n","        model, X_te, Y_te_norm, mu, sd, collate_fn,\n","        block_name=None, device=device\n","    )\n","    imp_blocks[\"fold\"] = fold_id\n","    imp_blocks_all.append(imp_blocks)\n","\n","    # (B) Per-feature permutation inside the numeric block\n","    if X_te[\"num\"].shape[1] > 0:\n","        _, imp_num = permutation_importance(\n","            model, X_te, Y_te_norm, mu, sd, collate_fn,\n","            block_name=\"num\", feature_names=NUMERIC_FEATURE_NAMES, device=device\n","        )\n","        imp_num[\"fold\"] = fold_id\n","        imp_weather_all.append(imp_num)   # or rename this collector to imp_num_all\n"]},{"cell_type":"markdown","source":["report 2 (per target evaluation)"],"metadata":{"id":"TpcWc8d5zRo3"},"id":"TpcWc8d5zRo3"},{"cell_type":"code","execution_count":null,"id":"IBGFIVH614vV","metadata":{"id":"IBGFIVH614vV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760599390675,"user_tz":-660,"elapsed":60,"user":{"displayName":"Sarah Liu","userId":"15845808413963530009"}},"outputId":"e285d52b-5329-418c-d5ef-fe2d75730d2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Per-target metrics for severity_count (outer 10-fold) ===\n","      rmse_count  mae_count  r2_count\n","fold                                 \n","1       0.046371   0.035472  0.817168\n","2       0.045154   0.034809  0.825144\n","3       0.050471   0.039288  0.786355\n","4       0.044601   0.034290  0.826208\n","5       0.047945   0.037719  0.799128\n","6       0.044266   0.035037  0.831521\n","7       0.046558   0.035449  0.808535\n","8       0.043475   0.032186  0.839865\n","9       0.043885   0.034321  0.826222\n","10      0.045750   0.034721  0.827158\n","Mean ± SE:\n","RMSE: 0.0458 ± 0.0007\n","MAE : 0.0353 ± 0.0006\n","R²  : 0.8187 ± 0.0051\n","\n","=== Per-target metrics for severity_mean (outer 10-fold) ===\n","      rmse_mean  mae_mean   r2_mean\n","fold                               \n","1      0.064777  0.027771  0.438807\n","2      0.065997  0.028497  0.519267\n","3      0.059477  0.026402  0.497732\n","4      0.065154  0.029949  0.552897\n","5      0.061635  0.028521  0.516438\n","6      0.060576  0.025767  0.543122\n","7      0.062115  0.027988  0.513465\n","8      0.059364  0.025461  0.564368\n","9      0.061749  0.027074  0.533967\n","10     0.065856  0.028957  0.561760\n","Mean ± SE:\n","RMSE: 0.0627 ± 0.0008\n","MAE : 0.0276 ± 0.0005\n","R²  : 0.5242 ± 0.0118\n","\n","=== Per-target metrics for severity_max (outer 10-fold) ===\n","      rmse_max   mae_max    r2_max\n","fold                              \n","1     0.596009  0.354202  0.484373\n","2     0.622965  0.364873  0.484699\n","3     0.589421  0.365519  0.489753\n","4     0.608471  0.357279  0.524785\n","5     0.623184  0.371486  0.460657\n","6     0.589661  0.343604  0.523340\n","7     0.597571  0.357368  0.478634\n","8     0.556437  0.309983  0.552153\n","9     0.577258  0.327176  0.516618\n","10    0.587313  0.331835  0.516133\n","Mean ± SE:\n","RMSE: 0.5948 ± 0.0064\n","MAE : 0.3483 ± 0.0062\n","R²  : 0.5031 ± 0.0088\n","\n","Most selected hyperparameters:\n","- d_token: 128\n","- n_layers: 3\n","- lr: 0.001\n","- dropout: 0.1\n","- n_heads: 8\n","- weight_decay: 1e-06\n"]}],"source":["res_df = pd.DataFrame(outer_results).set_index(\"fold\").sort_index()\n","mean = res_df.mean(numeric_only=True); se = res_df.std(ddof=1, numeric_only=True) / np.sqrt(len(res_df))\n","\n","for name in [\"count\", \"mean\", \"max\"]:\n","    print(f\"\\n=== Per-target metrics for severity_{name} (outer 10-fold) ===\")\n","    cols = [f\"rmse_{name}\", f\"mae_{name}\", f\"r2_{name}\"]\n","    print(res_df[cols])\n","\n","    print(\"Mean ± SE:\")\n","    print(f\"RMSE: {mean[f'rmse_{name}']:.4f} ± {se[f'rmse_{name}']:.4f}\")\n","    print(f\"MAE : {mean[f'mae_{name}']:.4f} ± {se[f'mae_{name}']:.4f}\")\n","    print(f\"R²  : {mean[f'r2_{name}']:.4f} ± {se[f'r2_{name}']:.4f}\")\n","\n","hp_df = pd.DataFrame(best_params_all)\n","print(\"\\nMost selected hyperparameters:\")\n","for col in hp_df.columns:\n","    try:\n","        print(f\"- {col}: {Counter(hp_df[col]).most_common(1)[0][0]}\")\n","    except Exception:\n","        pass"]},{"cell_type":"code","execution_count":null,"id":"MIEhNtTG2HqW","metadata":{"id":"MIEhNtTG2HqW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760599390694,"user_tz":-660,"elapsed":21,"user":{"displayName":"Sarah Liu","userId":"15845808413963530009"}},"outputId":"7a31be67-793b-44ee-b56c-bca4b43103fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Block permutation importance (ΔRMSE, mean over folds — higher = more important):\n","block\n","num          0.201438\n","county_id    0.000316\n","wcat_id      0.000110\n","city_id     -0.000068\n","Name: importance, dtype: float64\n","\n","Top NUMERIC features by ΔRMSE (mean over folds):\n","feature\n","Emb_PCA_0                       0.086090\n","Emb_PCA_4                       0.067645\n","Emb_PCA_6                       0.030021\n","Emb_PCA_2                       0.010963\n","Emb_PCA_3                       0.007446\n","Emb_PCA_8                       0.005987\n","Emb_PCA_5                       0.004327\n","Emb_PCA_1                       0.004063\n","Emb_PCA_10                      0.002963\n","Emb_PCA_31                      0.002634\n","Emb_PCA_14                      0.002264\n","Emb_PCA_23                      0.002112\n","Bump                            0.002097\n","Astronomical_Twilight_binary    0.002085\n","Emb_PCA_7                       0.001997\n","Amenity                         0.001978\n","Emb_PCA_22                      0.001971\n","Emb_PCA_24                      0.001735\n","Emb_PCA_42                      0.001638\n","Emb_PCA_26                      0.001630\n","Name: importance, dtype: float64\n"]}],"source":["# ---- Aggregate importance over folds ----\n","if len(imp_blocks_all):\n","    imp_blocks_df = pd.concat(imp_blocks_all, ignore_index=True)\n","    # columns are: ['block', 'importance', 'fold']\n","    print(\"\\nBlock permutation importance (ΔRMSE, mean over folds — higher = more important):\")\n","    print(\n","        imp_blocks_df.groupby(\"block\")[\"importance\"]\n","        .mean()\n","        .sort_values(ascending=False)\n","    )\n","\n","if len(imp_weather_all):\n","    # This list actually holds per-feature importance for the numeric block.\n","    # If you renamed it to imp_num_all, change the variable name accordingly.\n","    imp_num_df = pd.concat(imp_weather_all, ignore_index=True)\n","    # columns are: ['feature', 'importance', 'fold']\n","    print(\"\\nTop NUMERIC features by ΔRMSE (mean over folds):\")\n","    print(\n","        imp_num_df.groupby(\"feature\")[\"importance\"]\n","        .mean()\n","        .sort_values(ascending=False)\n","        .head(20)\n","    )\n","\n","# If you collected Grad×Input saliency per feature into saliency_all (a list of Series),\n","# this still works. Just make sure the Series index are numeric feature names.\n","if len(saliency_all):\n","    saliency_mat = pd.concat(saliency_all, axis=1)  # columns = folds\n","    print(\"\\nGrad×Input saliency (NUM) — mean over folds:\")\n","    print(saliency_mat.mean(axis=1).sort_values(ascending=False).head(20))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}